{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5cFq_TgWlQ_"
   },
   "source": [
    "# Homework 7 - Network Compression (Knowledge Distillation)\n",
    "\n",
    "> Author: Arvin Liu (b05902127@ntu.edu.tw)\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bpmQUZhukmqe",
    "outputId": "469e5ff7-fc16-4ee6-ca76-08608ae66b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delay/anaconda3/bin/gdown\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/gdown/cli.py\", line 105, in main\n",
      "    use_cookies=not args.no_cookies,\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/gdown/download.py\", line 110, in download\n",
      "    res = sess.get(url, stream=True)\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 543, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 376, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 994, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 300, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 157, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "unzip:  cannot find or open food-11.zip, food-11.zip.zip or food-11.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "!gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip\n",
    "# Unzip the files\n",
    "!unzip food-11.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNiZCGrIYKdR"
   },
   "source": [
    "# Readme\n",
    "\n",
    "\n",
    "HW7的任務是模型壓縮 - Neural Network Compression。\n",
    "\n",
    "Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n",
    "\n",
    "* 知識蒸餾 Knowledge Distillation\n",
    "* 網路剪枝 Network Pruning\n",
    "* 用少量參數來做CNN Architecture Design\n",
    "* 參數量化 Weight Quantization\n",
    "\n",
    "在這個notebook中我們會介紹Knowledge Distillation，\n",
    "而我們有提供已經學習好的大model方便大家做Knowledge Distillation。\n",
    "而我們使用的小model是\"Architecture Design\"過的model。\n",
    "\n",
    "* Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n",
    "* 下載pretrained大model(47.2M): https://drive.google.com/file/d/1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN/view?usp=sharing\n",
    "  * 請使用torchvision提供的ResNet18，把num_classes改成11後load進去即可。(後面有範例。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "XdzskhdEb65Z",
    "outputId": "d856fd31-a1e3-4653-b368-5958c55a869f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lJS0ApIyi7eZ2b3GMyGxjPShI8jXM2UC\n",
      "To: /content/hw7_Architecture_Design.ipynb\n",
      "\r",
      "  0% 0.00/8.78k [00:00<?, ?B/s]\r",
      "100% 8.78k/8.78k [00:00<00:00, 15.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "# Load進我們的Model架構(在hw7_Architecture_Design.ipynb內)\n",
    "!gdown --id '1lJS0ApIyi7eZ2b3GMyGxjPShI8jXM2UC' --output \"hw7_Architecture_Design.ipynb\"\n",
    "%run \"hw7_Architecture_Design.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdUtCxBBcH0B"
   },
   "source": [
    "Knowledge Distillation\n",
    "===\n",
    "\n",
    "<img src=\"https://i.imgur.com/H2aF7Rv.png=100x\" width=\"500px\">\n",
    "\n",
    "簡單上來說就是讓已經做得很好的大model們去告訴小model\"如何\"學習。\n",
    "而我們如何做到這件事情呢? 就是利用大model預測的logits給小model當作標準就可以了。\n",
    "\n",
    "## 為甚麼這會work?\n",
    "* 例如當data不是很乾淨的時候，對一般的model來說他是個noise，只會干擾學習。透過去學習其他大model預測的logits會比較好。\n",
    "* label和label之間可能有關連，這可以引導小model去學習。例如數字8可能就和6,9,0有關係。\n",
    "* 弱化已經學習不錯的target(?)，避免讓其gradient干擾其他還沒學好的task。\n",
    "\n",
    "\n",
    "## 要怎麼實作?\n",
    "* $Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{原本的Loss})$\n",
    "\n",
    "\n",
    "* 以下code為甚麼要對student使用log_softmax: https://github.com/peterliht/knowledge-distillation-pytorch/issues/2\n",
    "* reference: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-dSi_P-4les"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfnRoOt5VIze"
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "我們的Dataset使用的是跟Hw3 - CNN同樣的Dataset，因此這個區塊的Augmentation / Read Image大家參考或直接抄就好。\n",
    "\n",
    "如果有不會的話可以回去看Hw3的colab。\n",
    "\n",
    "需要注意的是如果要自己寫的話，Augment的方法最好使用我們的方法，避免輸入有差異導致Teacher Net預測不好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExdUvTRaVNOT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, folderName, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "        for img_path in sorted(glob(folderName + '/*.jpg')):\n",
    "            try:\n",
    "                # Get classIdx by parsing image path\n",
    "                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n",
    "            except:\n",
    "                # if inference mode (there's no answer), class_idx default 0\n",
    "                class_idx = 0\n",
    "\n",
    "            image = Image.open(img_path)\n",
    "            # Get File Descriptor\n",
    "            image_fp = image.fp\n",
    "            image.load()\n",
    "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
    "            image_fp.close()\n",
    "\n",
    "            self.data.append(image)\n",
    "            self.label.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label[idx]\n",
    "\n",
    "\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_dataloader(mode='training', batch_size=32):\n",
    "\n",
    "    assert mode in ['training', 'testing', 'validation']\n",
    "\n",
    "    dataset = MyDataset(\n",
    "        f'./food-11/{mode}',\n",
    "        transform=trainTransform if mode == 'training' else testTransform)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(mode == 'training'))\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACPwL9_JWceQ"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "我們已經提供TeacherNet的state_dict，其架構是torchvision提供的ResNet18。\n",
    "\n",
    "至於StudentNet的架構則在hw7_Architecture_Design.ipynb中。\n",
    "\n",
    "這裡我們使用的Optimizer為AdamW，沒有為甚麼，就純粹我想用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzuuGvnbWkG8"
   },
   "outputs": [],
   "source": [
    "# get dataloader\n",
    "train_dataloader = get_dataloader('training', batch_size=32)\n",
    "valid_dataloader = get_dataloader('validation', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ZWdQtDtgoGCp",
    "outputId": "dc49692a-c7e4-4838-a09b-d64ac9aae1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/delay/anaconda3/bin/gdown\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/gdown/cli.py\", line 105, in main\r\n",
      "    use_cookies=not args.no_cookies,\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/gdown/download.py\", line 110, in download\r\n",
      "    res = sess.get(url, stream=True)\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 543, in get\r\n",
      "    return self.request('GET', url, **kwargs)\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 530, in request\r\n",
      "    resp = self.send(prep, **send_kwargs)\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 643, in send\r\n",
      "    r = adapter.send(request, **kwargs)\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\r\n",
      "    timeout=timeout\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 672, in urlopen\r\n",
      "    chunked=chunked,\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 376, in _make_request\r\n",
      "    self._validate_conn(conn)\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 994, in _validate_conn\r\n",
      "    conn.connect()\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 300, in connect\r\n",
      "    conn = self._new_conn()\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 157, in _new_conn\r\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\r\n",
      "  File \"/home/delay/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\", line 74, in create_connection\r\n",
      "    sock.connect(sa)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-411ec5e415cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gdown --id '1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN' --output teacher_resnet18.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mteacher_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstudent_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStudentNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "!gdown --id '1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN' --output teacher_resnet18.bin\n",
    "\n",
    "teacher_net = models.resnet18(pretrained=False, num_classes=11).cuda()\n",
    "student_net = StudentNet(base=16).cuda()\n",
    "\n",
    "teacher_net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\n",
    "optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wvc1W5yO2QaE"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "* 剩下的步驟與你在做Hw3 - CNN的時候一樣。\n",
    "\n",
    "## 小提醒\n",
    "\n",
    "* torch.no_grad是指接下來的運算或該tensor不需要算gradient。\n",
    "* model.eval()與model.train()差在於Batchnorm要不要紀錄，以及要不要做Dropout。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-TzmWtT62Qmy",
    "outputId": "343e1c73-2e9b-43a4-82f0-004ce72b4842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train loss: 15.0419, acc 0.3214 valid loss: 15.2489, acc 0.3828\n",
      "epoch   1: train loss: 13.3112, acc 0.4125 valid loss: 14.1526, acc 0.4394\n",
      "epoch   2: train loss: 12.3028, acc 0.4670 valid loss: 13.3518, acc 0.5114\n",
      "epoch   3: train loss: 11.5357, acc 0.4951 valid loss: 12.0821, acc 0.5338\n",
      "epoch   4: train loss: 10.9616, acc 0.5178 valid loss: 13.1261, acc 0.5318\n",
      "epoch   5: train loss: 10.5729, acc 0.5371 valid loss: 11.9723, acc 0.5388\n",
      "epoch   6: train loss: 10.0196, acc 0.5624 valid loss: 10.5230, acc 0.5913\n",
      "epoch   7: train loss: 9.6870, acc 0.5767 valid loss: 10.1911, acc 0.5875\n",
      "epoch   8: train loss: 9.2314, acc 0.5910 valid loss: 10.2292, acc 0.6006\n",
      "epoch   9: train loss: 8.9352, acc 0.5991 valid loss: 10.2385, acc 0.6038\n",
      "epoch  10: train loss: 8.5889, acc 0.6201 valid loss: 8.8344, acc 0.6233\n",
      "epoch  11: train loss: 8.3348, acc 0.6274 valid loss: 8.3698, acc 0.6382\n",
      "epoch  12: train loss: 8.2765, acc 0.6343 valid loss: 9.0270, acc 0.6566\n",
      "epoch  13: train loss: 7.9758, acc 0.6464 valid loss: 7.9875, acc 0.6583\n",
      "epoch  14: train loss: 7.8688, acc 0.6508 valid loss: 8.1525, acc 0.6647\n",
      "epoch  15: train loss: 7.6558, acc 0.6511 valid loss: 8.5867, acc 0.6539\n",
      "epoch  16: train loss: 7.5757, acc 0.6562 valid loss: 7.6015, acc 0.6825\n",
      "epoch  17: train loss: 7.4253, acc 0.6683 valid loss: 8.0894, acc 0.6650\n",
      "epoch  18: train loss: 7.2671, acc 0.6715 valid loss: 9.2230, acc 0.6443\n",
      "epoch  19: train loss: 7.1529, acc 0.6804 valid loss: 7.0299, acc 0.6962\n",
      "epoch  20: train loss: 7.0817, acc 0.6812 valid loss: 7.6748, acc 0.6726\n",
      "epoch  21: train loss: 6.8803, acc 0.6917 valid loss: 6.6860, acc 0.7073\n",
      "epoch  22: train loss: 6.8224, acc 0.6953 valid loss: 6.8080, acc 0.6959\n",
      "epoch  23: train loss: 6.7055, acc 0.6954 valid loss: 6.6848, acc 0.7277\n",
      "epoch  24: train loss: 6.5530, acc 0.7086 valid loss: 7.1390, acc 0.7067\n",
      "epoch  25: train loss: 6.4072, acc 0.7131 valid loss: 6.5056, acc 0.7294\n",
      "epoch  26: train loss: 6.5662, acc 0.7113 valid loss: 6.8638, acc 0.7067\n",
      "epoch  27: train loss: 6.4499, acc 0.7112 valid loss: 7.4186, acc 0.6948\n",
      "epoch  28: train loss: 6.3107, acc 0.7190 valid loss: 6.0342, acc 0.7411\n",
      "epoch  29: train loss: 6.2760, acc 0.7177 valid loss: 6.4239, acc 0.7335\n",
      "epoch  30: train loss: 6.1902, acc 0.7243 valid loss: 6.0680, acc 0.7350\n",
      "epoch  31: train loss: 6.1389, acc 0.7273 valid loss: 6.6766, acc 0.7070\n",
      "epoch  32: train loss: 6.0988, acc 0.7298 valid loss: 6.3349, acc 0.7257\n",
      "epoch  33: train loss: 6.0306, acc 0.7297 valid loss: 6.9381, acc 0.7012\n",
      "epoch  34: train loss: 5.9296, acc 0.7373 valid loss: 6.8347, acc 0.7169\n",
      "epoch  35: train loss: 5.8375, acc 0.7402 valid loss: 6.6867, acc 0.7032\n",
      "epoch  36: train loss: 5.7554, acc 0.7399 valid loss: 5.9309, acc 0.7350\n",
      "epoch  37: train loss: 5.6957, acc 0.7430 valid loss: 6.7278, acc 0.7230\n",
      "epoch  38: train loss: 5.7600, acc 0.7432 valid loss: 6.0643, acc 0.7437\n",
      "epoch  39: train loss: 5.5869, acc 0.7516 valid loss: 5.9854, acc 0.7312\n",
      "epoch  40: train loss: 5.5545, acc 0.7515 valid loss: 6.4011, acc 0.7219\n",
      "epoch  41: train loss: 5.6061, acc 0.7475 valid loss: 6.2494, acc 0.7350\n",
      "epoch  42: train loss: 5.5099, acc 0.7620 valid loss: 6.1701, acc 0.7271\n",
      "epoch  43: train loss: 5.4721, acc 0.7573 valid loss: 5.8452, acc 0.7475\n",
      "epoch  44: train loss: 5.3817, acc 0.7610 valid loss: 5.5846, acc 0.7534\n",
      "epoch  45: train loss: 5.4271, acc 0.7657 valid loss: 5.5946, acc 0.7525\n",
      "epoch  46: train loss: 5.3696, acc 0.7604 valid loss: 5.4905, acc 0.7525\n",
      "epoch  47: train loss: 5.2922, acc 0.7717 valid loss: 6.3426, acc 0.7347\n",
      "epoch  48: train loss: 5.2458, acc 0.7727 valid loss: 5.2565, acc 0.7618\n",
      "epoch  49: train loss: 5.2409, acc 0.7709 valid loss: 5.3686, acc 0.7496\n",
      "epoch  50: train loss: 5.1905, acc 0.7741 valid loss: 5.2408, acc 0.7691\n",
      "epoch  51: train loss: 5.0879, acc 0.7740 valid loss: 5.1569, acc 0.7767\n",
      "epoch  52: train loss: 5.0245, acc 0.7786 valid loss: 5.4747, acc 0.7662\n",
      "epoch  53: train loss: 5.0851, acc 0.7751 valid loss: 5.3644, acc 0.7598\n",
      "epoch  54: train loss: 5.0145, acc 0.7755 valid loss: 5.4192, acc 0.7615\n",
      "epoch  55: train loss: 4.9962, acc 0.7852 valid loss: 5.3904, acc 0.7627\n",
      "epoch  56: train loss: 4.9306, acc 0.7871 valid loss: 5.9268, acc 0.7554\n",
      "epoch  57: train loss: 4.8489, acc 0.7906 valid loss: 6.2777, acc 0.7157\n",
      "epoch  58: train loss: 4.8592, acc 0.7858 valid loss: 5.5397, acc 0.7630\n",
      "epoch  59: train loss: 4.7994, acc 0.7888 valid loss: 6.2838, acc 0.7382\n",
      "epoch  60: train loss: 4.8407, acc 0.7853 valid loss: 5.6853, acc 0.7423\n",
      "epoch  61: train loss: 4.8490, acc 0.7897 valid loss: 5.6883, acc 0.7668\n",
      "epoch  62: train loss: 4.6894, acc 0.7989 valid loss: 5.4785, acc 0.7577\n",
      "epoch  63: train loss: 4.7033, acc 0.8007 valid loss: 5.1770, acc 0.7606\n",
      "epoch  64: train loss: 4.7178, acc 0.7957 valid loss: 5.8482, acc 0.7507\n",
      "epoch  65: train loss: 4.7344, acc 0.7989 valid loss: 5.2141, acc 0.7671\n",
      "epoch  66: train loss: 4.6471, acc 0.7993 valid loss: 5.4845, acc 0.7531\n",
      "epoch  67: train loss: 4.6181, acc 0.8048 valid loss: 4.8036, acc 0.7822\n",
      "epoch  68: train loss: 4.6554, acc 0.7991 valid loss: 5.1246, acc 0.7673\n",
      "epoch  69: train loss: 4.5425, acc 0.7980 valid loss: 5.0636, acc 0.7746\n",
      "epoch  70: train loss: 4.5073, acc 0.8033 valid loss: 5.4564, acc 0.7528\n",
      "epoch  71: train loss: 4.5248, acc 0.8019 valid loss: 5.0240, acc 0.7746\n",
      "epoch  72: train loss: 4.5022, acc 0.8133 valid loss: 4.7639, acc 0.7773\n",
      "epoch  73: train loss: 4.5834, acc 0.8009 valid loss: 4.9609, acc 0.7636\n",
      "epoch  74: train loss: 4.4927, acc 0.8113 valid loss: 4.9222, acc 0.7799\n",
      "epoch  75: train loss: 4.4423, acc 0.8164 valid loss: 5.3646, acc 0.7665\n",
      "epoch  76: train loss: 4.3856, acc 0.8153 valid loss: 4.9068, acc 0.7819\n",
      "epoch  77: train loss: 4.3538, acc 0.8110 valid loss: 5.0977, acc 0.7665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9613b6df4631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mstudent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mstudent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9613b6df4631>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(dataloader, update, alpha)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_kd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoft_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtotal_hit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhard_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtotal_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_epoch(dataloader, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader):\n",
    "        # 清空 optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # 處理 input\n",
    "        inputs, hard_labels = batch_data\n",
    "        inputs = inputs.cuda()\n",
    "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
    "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
    "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
    "        with torch.no_grad():\n",
    "            soft_labels = teacher_net(inputs)\n",
    "\n",
    "        if update:\n",
    "            logits = student_net(inputs)\n",
    "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
    "            # T=20是原始論文的參數設定。\n",
    "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        else:\n",
    "            # 只是算validation acc的話，就開no_grad節省空間。\n",
    "            with torch.no_grad():\n",
    "                logits = student_net(inputs)\n",
    "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            \n",
    "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
    "        total_num += len(inputs)\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    return total_loss / total_num, total_hit / total_num\n",
    "\n",
    "\n",
    "# TeacherNet永遠都是Eval mode.\n",
    "teacher_net.eval()\n",
    "now_best_acc = 0\n",
    "for epoch in range(200):\n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n",
    "        epoch, train_loss, train_acc, valid_loss, valid_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GObCiGNtPkZ"
   },
   "source": [
    "# Inference\n",
    "\n",
    "同Hw3，請參考該作業:)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIcblvbUCTOP"
   },
   "source": [
    "# Q&A\n",
    "\n",
    "有任何問題Network Compression的問題可以寄信到b05902127@ntu.edu.tw / ntu-ml-2020spring-ta@googlegroups.com。\n",
    "\n",
    "時間允許的話我會更新在這裡。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "hw7_Knowledge_Distillation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
