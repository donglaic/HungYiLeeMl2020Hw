{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXMVwO8CRbqo"
   },
   "source": [
    "# Homework 7 - Network Compression (Weight Quantization)\n",
    "\n",
    "> Author: Arvin Liu (b05902127@ntu.edu.tw)\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Hwg2LOQRaee"
   },
   "source": [
    "# Readme\n",
    "\n",
    "\n",
    "HW7的任務是模型壓縮 - Neural Network Compression。\n",
    "\n",
    "Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n",
    "\n",
    "* 知識蒸餾 Knowledge Distillation\n",
    "* 網路剪枝 Network Pruning\n",
    "* 用少量參數來做CNN Architecture Design\n",
    "* 參數量化 Weight Quantization\n",
    "\n",
    "在這個notebook中我們會介紹非常簡單的Weight Quantization，\n",
    "而我們有提供已經做完Knowledge Distillation的小model來做Quantization。\n",
    "\n",
    "* Model架構 / Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n",
    "* 下載已經train好的小model(0.99M): https://drive.google.com/open?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\n",
    "  * 參數為 base=16, width_mult=1 (default)\n",
    "\n",
    "\n",
    "## Weight Quantization\n",
    "<img src=\"https://i.imgur.com/SMsaiAo.png\" width=\"500px\">\n",
    "\n",
    "我們這邊會示範如何實作第一條: Using less bits to represent a value。\n",
    "\n",
    "## 好的Quantization很重要。\n",
    "這邊提供一些TA的數據供各位參考。\n",
    "\n",
    "|bit|state_dict size|accuracy|\n",
    "|-|-|-|\n",
    "|32|1047430 Bytes|0.81315|\n",
    "|16|522958 Bytes|0.81347|\n",
    "|8|268472 Bytes|0.80791|\n",
    "|7|268472 Bytes|0.80791|\n",
    "\n",
    "\n",
    "## Byte Cost\n",
    "根據[torch的官方手冊](https://pytorch.org/docs/stable/tensors.html)，我們知道torch.FloatTensor預設是32-bit，也就是佔了4byte的空間，而FloatTensor系列最低可以容忍的是16-bit。\n",
    "\n",
    "為了方便操作，我們之後會將state_dict轉成numpy array做事。\n",
    "因此我們可以先看看numpy有甚麼樣的type可以使用。\n",
    "![](https://i.imgur.com/3N7tiEc.png)\n",
    "而我們發現numpy最低有float16可以使用，因此我們可以直接靠轉型將32-bit的tensor轉換成16-bit的ndarray存起來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6PTWn7LdvMn"
   },
   "source": [
    "# Read state_dict\n",
    "\n",
    "下載我們已經train好的小model的state_dict進行測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "XRe3T_Uwd29U",
    "outputId": "33a396d4-1277-4352-a028-de7eee9825d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\n",
      "To: /content/student_custom_small.bin\n",
      "\r",
      "  0% 0.00/1.05M [00:00<?, ?B/s]\r",
      "100% 1.05M/1.05M [00:00<00:00, 69.9MB/s]\n",
      "\n",
      "original cost: 1047430 bytes.\n"
     ]
    }
   ],
   "source": [
    "!gdown --id '12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL' --output student_custom_small.bin\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"\\noriginal cost: {os.stat('student_custom_small.bin').st_size} bytes.\")\n",
    "params = torch.load('student_custom_small.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXD0_G5McRGt"
   },
   "source": [
    "# 32-bit Tensor -> 16-bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OIV0BzszcQg8",
    "outputId": "6b97bce2-0a1a-400d-ae65-5fdfce1c9fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-bit cost: 522958 bytes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def encode16(params, fname):\n",
    "    '''將params壓縮成16-bit後輸出到fname。\n",
    "\n",
    "    Args:\n",
    "      params: model的state_dict。\n",
    "      fname: 壓縮後輸出的檔名。\n",
    "    '''\n",
    "\n",
    "    custom_dict = {}\n",
    "    for (name, param) in params.items():\n",
    "        param = np.float64(param.cpu().numpy())\n",
    "        # 有些東西不屬於ndarray，只是一個數字，這個時候我們就不用壓縮。\n",
    "        if type(param) == np.ndarray:\n",
    "            custom_dict[name] = np.float16(param)\n",
    "        else:\n",
    "            custom_dict[name] = param\n",
    "\n",
    "    pickle.dump(custom_dict, open(fname, 'wb'))\n",
    "\n",
    "\n",
    "def decode16(fname):\n",
    "    '''從fname讀取各個params，將其從16-bit還原回torch.tensor後存進state_dict內。\n",
    "\n",
    "    Args:\n",
    "      fname: 壓縮後的檔名。\n",
    "    '''\n",
    "\n",
    "    params = pickle.load(open(fname, 'rb'))\n",
    "    custom_dict = {}\n",
    "    for (name, param) in params.items():\n",
    "        param = torch.tensor(param)\n",
    "        custom_dict[name] = param\n",
    "\n",
    "    return custom_dict\n",
    "\n",
    "\n",
    "encode16(params, '16_bit_model.pkl')\n",
    "print(f\"16-bit cost: {os.stat('16_bit_model.pkl').st_size} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv5PtFoWgIFb"
   },
   "source": [
    "# 32-bit Tensor -> 8-bit (OPTIONAL)\n",
    "\n",
    "這邊提供轉成8-bit的方法，僅供大家參考。\n",
    "因為沒有8-bit的float，所以我們先對每個weight記錄最小值和最大值，進行min-max正規化後乘上$2^8-1$在四捨五入，就可以用np.uint8存取了。\n",
    "\n",
    "$W' = round(\\frac{W - \\min(W)}{\\max(W) - \\min(W)} \\times (2^8 - 1)$)\n",
    "\n",
    "\n",
    "\n",
    "> 至於能不能轉成更低的形式，例如4-bit呢? 當然可以，待你實作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2vh9Pn-3hZEN",
    "outputId": "91513e99-2e7e-4736-eea9-73bb2a624523"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1bd4ba61173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mencode8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8_bit_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"8-bit cost: {os.stat('8_bit_model.pkl').st_size} bytes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "def encode8(params, fname):\n",
    "    custom_dict = {}\n",
    "    for (name, param) in params.items():\n",
    "        param = np.float64(param.cpu().numpy())\n",
    "        if type(param) == np.ndarray:\n",
    "            min_val = np.min(param)\n",
    "            max_val = np.max(param)\n",
    "            param = np.round((param - min_val) / (max_val - min_val) * 255)\n",
    "            param = np.uint8(param)\n",
    "            custom_dict[name] = (min_val, max_val, param)\n",
    "        else:\n",
    "            custom_dict[name] = param\n",
    "\n",
    "    pickle.dump(custom_dict, open(fname, 'wb'))\n",
    "\n",
    "\n",
    "def decode8(fname):\n",
    "    params = pickle.load(open(fname, 'rb'))\n",
    "    custom_dict = {}\n",
    "    for (name, param) in params.items():\n",
    "        if type(param) == tuple:\n",
    "            min_val, max_val, param = param\n",
    "            param = np.float64(param)\n",
    "            param = (param / 255 * (max_val - min_val)) + min_val\n",
    "            param = torch.tensor(param)\n",
    "        else:\n",
    "            param = torch.tensor(param)\n",
    "\n",
    "        custom_dict[name] = param\n",
    "\n",
    "    return custom_dict\n",
    "\n",
    "encode8(params, '8_bit_model.pkl')\n",
    "print(f\"8-bit cost: {os.stat('8_bit_model.pkl').st_size} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0M7JRXSjQwa"
   },
   "source": [
    "# Q&A\n",
    "\n",
    "有任何問題Network Compression的問題可以寄信到b05902127@ntu.edu.tw。\n",
    "\n",
    "時間允許的話我會更新在這裡。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw7_Weight_Quantization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
